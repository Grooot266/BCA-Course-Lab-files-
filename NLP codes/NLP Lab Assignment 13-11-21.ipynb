{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f487252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP ASSIGNMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c7a2ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last\n",
      "\n",
      "week\n",
      "\n",
      ",\n",
      "\n",
      "the\n",
      "\n",
      "University\n",
      "\n",
      "of\n",
      "\n",
      "Cambridge\n",
      "\n",
      "shared\n",
      "\n",
      "its\n",
      "\n",
      "own\n",
      "\n",
      "research\n",
      "\n",
      "that\n",
      "\n",
      "shows\n",
      "\n",
      "if\n",
      "\n",
      "everyone\n",
      "\n",
      "wears\n",
      "\n",
      "a\n",
      "\n",
      "mask\n",
      "\n",
      "outside\n",
      "\n",
      "home\n",
      "\n",
      ",\n",
      "\n",
      "dreaded\n",
      "\n",
      "‘\n",
      "\n",
      "second\n",
      "\n",
      "wave\n",
      "\n",
      "’\n",
      "\n",
      "of\n",
      "\n",
      "the\n",
      "\n",
      "pandemic\n",
      "\n",
      "can\n",
      "\n",
      "be\n",
      "\n",
      "avoided\n",
      "\n",
      ".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Q1 Tokenize a given text(word tokenization)\n",
    "# text=\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home, dreaded ‘second wave’ of the pandemic can be avoided.\"\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "text =\"Last week, the University of Cambridge shared its own research that shows if everyone wears a mask outside home, dreaded ‘second wave’ of the pandemic can be avoided.\"\n",
    "sentences=word_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "909adee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Porter stemmer the stem words are:-\n",
      "data\n",
      "scienc\n",
      "is\n",
      "an\n",
      "inter-disciplinari\n",
      "field\n",
      "that\n",
      "use\n",
      "scientif\n",
      "method\n",
      ",\n",
      "process\n",
      ",\n",
      "algorithm\n",
      "and\n",
      "system\n",
      "to\n",
      "extract\n",
      "knowledg\n",
      "and\n",
      "insight\n",
      "from\n",
      "mani\n",
      "structur\n",
      "and\n",
      "unstructur\n",
      "data\n",
      "\n",
      "\n",
      "List of words which are not stopwords:-\n",
      "['data'] \n",
      "\n",
      "Length of words:- \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Q2  Using Porter stemmer find out the stem words from the sentence below and also remove stop words.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords \n",
    "ps = PorterStemmer()\n",
    "Text= \"Data science is an inter-disciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from many structural and unstructured data\" \n",
    "words = word_tokenize(Text)\n",
    "print(\"Using Porter stemmer the stem words are:-\")\n",
    "for w in words:\n",
    "        otp = ps.stem(w)\n",
    "        print(otp)\n",
    "stopwords = set(stopwords.words('english'))\n",
    "words = word_tokenize(otp.lower())\n",
    "wordsFiltered = []\n",
    "for w in words:\n",
    "        if w not in stopwords:\n",
    "            wordsFiltered.append(w)\n",
    "\n",
    "            \n",
    "print(\"\\n\")        \n",
    "\n",
    "print(\"List of words which are not stopwords:-\")\n",
    "print(wordsFiltered,\"\\n\")\n",
    "print(\"Length of words:- \")\n",
    "print(len(wordsFiltered))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edce3cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 6)\t1\n",
      "  (1, 0)\t2 \n",
      "\n",
      "['china', 'great', 'in', 'india', 'is', 'mahal', 'of', 'place', 'taj', 'tourist', 'wall'] \n",
      "\n",
      "[[0 0 1 1 1 1 0 1 1 1 0]\n",
      " [2 1 1 0 1 0 1 1 0 1 1]] \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>text{i+i}</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.455842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text{i+i}</th>\n",
       "      <td>0.455842</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              text1     text2\n",
       "text{i+i}  1.000000  0.455842\n",
       "text{i+i}  0.455842  1.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3 Find similarity between two documents given below(cosine similarity)\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "text1 = \"Taj Mahal is a tourist place in India\"\n",
    "text2 = \"Great Wall of China is a tourist place in china\"\n",
    "data = [text1, text2]\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "vector_matrix = count_vectorizer.fit_transform(data)\n",
    "print(vector_matrix,\"\\n\")\n",
    "\n",
    "\n",
    "tokens = count_vectorizer.get_feature_names()\n",
    "print(tokens,\"\\n\")\n",
    "\n",
    "\n",
    "print(vector_matrix.toarray(),\"\\n\")\n",
    "\n",
    "def create_dataframe(matrix, tokens):\n",
    "    \n",
    "    doc_names = ['text{i+i}' for i, _ in enumerate(matrix)]\n",
    "    df = pd.DataFrame(data=matrix, index= doc_names, columns=tokens)\n",
    "    return(df)\n",
    "\n",
    "create_dataframe(vector_matrix.toarray(),tokens)\n",
    "\n",
    "cosine_similarity_matrix = cosine_similarity(vector_matrix)\n",
    "create_dataframe(cosine_similarity_matrix,['text1','text2'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7c224cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word list of the text:-\n",
      "['Walter', 'was', 'feeling', 'anxious', '.', 'He', 'was', 'diagnosed', 'today', '.', 'He', 'probably', 'is', 'the', 'best', 'person', 'I', 'know', '.']\n",
      "Lemmatization:-\n",
      "Walter\n",
      "wa\n",
      "feeling\n",
      "anxious\n",
      ".\n",
      "He\n",
      "wa\n",
      "diagnosed\n",
      "today\n",
      ".\n",
      "He\n",
      "probably\n",
      "is\n",
      "the\n",
      "best\n",
      "person\n",
      "I\n",
      "know\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Q4 Given a text, perform lemmatization.\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "text =\"Walter was feeling anxious. He was diagnosed today. He probably is the best person I know.\"\n",
    "lemm=WordNetLemmatizer()\n",
    "print(\"Word list of the text:-\")\n",
    "word_list = nltk.word_tokenize(text)\n",
    "print(word_list)\n",
    "print(\"Lemmatization:-\")\n",
    "for w in word_list:\n",
    "    output=lemm.lemmatize(w)\n",
    "    print(output)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "200ebc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos-tags:-\n",
      "[('Donald', 'NNP'), ('John', 'NNP'), ('Trump', 'NNP'), ('is', 'VBZ'), ('an', 'DT'), ('American', 'JJ'), ('media', 'NNS'), ('personality', 'NN'), ('and', 'CC'), ('businessman', 'NN'), ('who', 'WP'), ('served', 'VBD'), ('as', 'IN'), ('the', 'DT'), ('45th', 'CD'), ('president', 'NN'), ('of', 'IN'), ('the', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('from', 'IN'), ('2017', 'CD'), ('to', 'TO'), ('2021', 'CD'), ('.', '.')]\n",
      "After Regex:- chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<V.*>*<CC>?'>\n",
      "After chunking:- (S\n",
      "  (mychunk Donald/NNP John/NNP Trump/NNP is/VBZ)\n",
      "  an/DT\n",
      "  (mychunk American/JJ)\n",
      "  (mychunk media/NNS personality/NN and/CC)\n",
      "  (mychunk businessman/NN)\n",
      "  who/WP\n",
      "  (mychunk served/VBD)\n",
      "  as/IN\n",
      "  the/DT\n",
      "  45th/CD\n",
      "  (mychunk president/NN)\n",
      "  of/IN\n",
      "  the/DT\n",
      "  (mychunk United/NNP)\n",
      "  States/NNPS\n",
      "  from/IN\n",
      "  2017/CD\n",
      "  to/TO\n",
      "  2021/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Q5 \n",
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text= \"Donald John Trump is an American media personality and businessman who served as the 45th president of the United States from 2017 to 2021.\"\n",
    "tokens=word_tokenize(text)\n",
    "mo=pos_tag(tokens)\n",
    "print(\"Pos-tags:-\")\n",
    "print(mo)\n",
    "pattern = \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<V.*>*<CC>?}\"\"\"\n",
    "chunks = RegexpParser(pattern)\n",
    "print(\"After Regex:-\",chunks)\n",
    "op=chunks.parse(mo)\n",
    "print(\"After chunking:-\",op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54cddcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases:-  ['Walter', 'Google', 'He', 'London']\n",
      "Verbs:- ['work', 'live']\n",
      "Walter PERSON\n",
      "London GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = (\"Walter works in Google. He lives in London \")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "print(\"Noun phrases:- \" , [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:-\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6a63f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
