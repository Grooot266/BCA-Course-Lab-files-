{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d30b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "352ffd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 'love', 'to')\n",
      "('love', 'to', 'play')\n",
      "('to', 'play', 'Battlegrounds')\n",
      "('play', 'Battlegrounds', 'Mobile')\n",
      "('Battlegrounds', 'Mobile', 'India')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence= \"I love to play Battlegrounds Mobile India\"\n",
    "n=3\n",
    "for gram in ngrams(word_tokenize(sentence),n):\n",
    "    print(gram)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c6cdc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "swimming ------> swim\n",
      "cycling ------> cycl\n",
      "gamming ------> gam\n",
      "player ------> player\n",
      "hackers ------> hacker\n",
      "hacks ------> hack\n",
      "vision ------> vision\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "words=['swimming','cycling','gamming','player','hackers','hacks','vision']\n",
    "stem_words = []\n",
    "for w in words:\n",
    "    x= snow_stemmer.stem(w)\n",
    "    stem_words.append(x)\n",
    "for e1,e2 in zip(words,stem_words):\n",
    "    print(e1+' ------> ' +e2)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9fe0962",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7eb0e161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                Porter Stemmer      lancaster Stemmer   Snowball Stemmer    \n",
      "friend              friend              friend              friend              \n",
      "friendship          friendship          friend              friendship          \n",
      "friends             friend              friend              friend              \n",
      "friendships         friendship          friend              friendship          \n",
      "stabil              stabil              stabl               stabil              \n",
      "destabilize         destabil            dest                destabil            \n",
      "misunderstanding    misunderstand       misunderstand       misunderstand       \n",
      "railroad            railroad            railroad            railroad            \n",
      "moonlight           moonlight           moonlight           moonlight           \n",
      "football            footbal             footbal             footbal             \n"
     ]
    }
   ],
   "source": [
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snow_stemmer = SnowballStemmer(language='english')\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "print(\"{0:20}{1:20}{2:20}{3:20}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\",\"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20}{1:20}{2:20}{3:20}\".format(word,porter.stem(word),lancaster.stem(word),snow_stemmer.stem(word)))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7675e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\prathmesha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#wordnet library requires lematization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e9e3b27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increses\n",
      "foot\n",
      "teeth\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemm=WordNetLemmatizer()\n",
    "print(lemm.lemmatize(\"increses\"))\n",
    "print(lemm.lemmatize(\"feet\"))\n",
    "print(lemm.lemmatize(\"teeth\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a51a6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "word_list = [\"am\",\"is\",\"are\",\"was\",\"were\"]\n",
    "for w in word_list:\n",
    "    print(lemma.lemmatize(w,pos=\"v\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc46c68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study\n",
      "studying\n",
      "studying\n",
      "studying\n"
     ]
    }
   ],
   "source": [
    "#v, n, a and r are pos tags where r = adverb  a=adjective v= verb n=noun\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize('studying', pos=\"v\"))\n",
    "print(lemmatizer.lemmatize('studying', pos=\"n\"))\n",
    "print(lemmatizer.lemmatize('studying', pos=\"a\"))\n",
    "print(lemmatizer.lemmatize('studying', pos=\"r\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ee1b5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "study\n",
      "leaf\n",
      "decrease\n",
      "play\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "word_list = [\"studies\",\"leaves\",\"decreases\",\"plays\"]\n",
    "for w in word_list:\n",
    "    print(lemma.lemmatize(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "792d11ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\prathmesha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02784831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('An', 'DT'), ('English', 'JJ'), ('sentence', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('group', 'NN'), ('of', 'IN'), ('words', 'NNS'), ('containing', 'VBG'), ('at', 'IN'), ('least', 'JJS'), ('one', 'CD'), ('subject', 'NN'), ('and', 'CC'), ('one', 'CD'), ('verb', 'NN'), ('that', 'WDT'), ('constructs', 'VBZ'), ('a', 'DT'), ('sensible', 'JJ'), ('unit', 'NN'), ('of', 'IN'), ('language', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "text= \"An English sentence is a group of words containing at least one subject and one verb that constructs a sensible unit of language.\"\n",
    "tokens=word_tokenize(text)\n",
    "mo=pos_tag(tokens)\n",
    "print(mo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56548b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  In/IN\n",
      "  (NP every/DT)\n",
      "  vaccination/NN\n",
      "  certificate/NN\n",
      "  there/EX\n",
      "  is/VBZ\n",
      "  (NP a/DT eminent/JJ)\n",
      "  scientist/NN)\n"
     ]
    }
   ],
   "source": [
    "#Chunking\n",
    "import nltk\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "s='In every vaccination certificate there is a eminent scientist'\n",
    "s=nltk.pos_tag(word_tokenize(s))\n",
    "chunk_rule=r\"NP:{<DT>?<JJ>*}\"\n",
    "chunk_parser=nltk.RegexpParser(chunk_rule)\n",
    "res=chunk_parser.parse(s)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5401bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Can', 'MD'), ('you', 'PRP'), ('help', 'VB'), ('me', 'PRP'), ('Groot', 'NNP'), ('?', '.')]\n",
      "After Extracting\n",
      " (S Can/MD you/PRP (VP (V help/VB)) me/PRP Groot/NNP ?/.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag, word_tokenize, RegexpParser\n",
    "sample_text = \"Can you help me Groot ?\"\n",
    "tagged = pos_tag(word_tokenize(sample_text))\n",
    "print(tagged)\n",
    "chunker= RegexpParser(\"\"\"\n",
    "NP: {<DT>?<JJ>*<NN>} #To extract Noun Phrases\n",
    "P: {<IN>} #To extract Prepositions\n",
    "V: {<V.*>} #To extract Verbs\n",
    "PP: {<P> <NP>} #To extract Prepostional Phrases\n",
    "VP: {<V> <NP|PP>*} #To extarct Verb Phrases\n",
    "\"\"\")\n",
    "output = chunker.parse(tagged)\n",
    "print(\"After Extracting\\n\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b9192",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
